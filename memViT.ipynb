{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import multiprocessing\n",
    "import math\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from filelock import FileLock\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops_exts import repeat_many\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from memorizing_transformers_pytorch.knn_memory import KNNMemoryList, DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "#torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(t):\n",
    "    return t\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def unique(arr):\n",
    "    return list({el: True for el in arr}.keys())\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cast_tuple(val, length = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * length)\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, dim = -1)\n",
    "\n",
    "def stable_softmax(t, dim = -1):\n",
    "    t = t - t.amax(dim = dim, keepdim = True).detach()\n",
    "    return F.softmax(t, dim = dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :] \n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
    "    return pe.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        out = self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "        if not isinstance(out, tuple):\n",
    "            return out + x\n",
    "\n",
    "        head, *tail = out\n",
    "        return (head + x, *tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5 relative positional bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5RelativePositionBias(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale,\n",
    "        num_buckets = 32,\n",
    "        max_distance = 128,\n",
    "        heads = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(\n",
    "        relative_position,\n",
    "        num_buckets = 32,\n",
    "        max_distance = 128\n",
    "    ):\n",
    "        n = -relative_position\n",
    "        n = torch.max(n, torch.zeros_like(n))\n",
    "\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "        return torch.where(is_small, n, val_if_large)\n",
    "\n",
    "    def forward(self, i, j, *, device):\n",
    "        q_pos = torch.arange(i, dtype = torch.long, device = device)\n",
    "        k_pos = torch.arange(j, dtype = torch.long, device = device)\n",
    "        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n",
    "        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n",
    "        values = self.relative_attention_bias(rp_bucket)\n",
    "        bias = rearrange(values, 'i j h -> () h i j')\n",
    "        return bias * self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        xl_max_memories = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.xl_max_memories = xl_max_memories\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim) #vit\n",
    "        self.attend = nn.Softmax(dim = -1) #vit\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x, *, xl_memory = None, rel_pos_bias = None):\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        h, device = self.heads, x.device\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        if exists(xl_memory):\n",
    "            k_xl_mem, v_xl_mem = xl_memory.unbind(dim = -2)\n",
    "            k = torch.cat((k_xl_mem, k), dim = -2)\n",
    "            v = torch.cat((v_xl_mem, v), dim = -2)\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k)\n",
    "        i, j = sim.shape[-2:]\n",
    "\n",
    "        if exists(rel_pos_bias):\n",
    "            sim = rel_pos_bias[..., -i:, -j:] + sim\n",
    "\n",
    "        causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n",
    "        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        attn = stable_softmax(sim)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        # new xl memories\n",
    "\n",
    "        new_kv_memories = torch.stack((k, v), dim = -2).detach()\n",
    "\n",
    "        if self.xl_max_memories > 0:\n",
    "            new_xl_kv_memories = new_kv_memories[:, -self.xl_max_memories:]\n",
    "        else:\n",
    "            new_xl_kv_memories = None\n",
    "\n",
    "        return self.to_out(out), new_xl_kv_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# approximate nearest neighbor attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        num_retrieved_memories = 32,\n",
    "        xl_max_memories = 0.,\n",
    "        attn_scale_init = 20,\n",
    "        gate_output = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = nn.Parameter(torch.ones(heads, 1, 1) * math.log(attn_scale_init))\n",
    "\n",
    "        inner_dim = heads * dim_head\n",
    "        self.xl_max_memories = xl_max_memories\n",
    "\n",
    "        self.num_retrieved_memories = num_retrieved_memories\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.knn_mem_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "        self.output_gate = nn.Parameter(torch.zeros(1)) if gate_output else None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        *,\n",
    "        knn_memory,\n",
    "        xl_memory = None,\n",
    "        add_knn_memory = True,\n",
    "        rel_pos_bias = None\n",
    "    ):\n",
    "        b, n, h, device = *x.shape[:2], self.heads, x.device\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        # in paper, they showed normalizing of keys led to more stable training\n",
    "        # we'll just go with full cosine sim attention https://arxiv.org/abs/2010.04245\n",
    "\n",
    "        q, k = map(l2norm, (q, k))\n",
    "\n",
    "        # handle xl memory\n",
    "\n",
    "        if exists(xl_memory):\n",
    "            k_xl_mem, v_xl_mem = xl_memory.unbind(dim = -2)\n",
    "            k = torch.cat((k_xl_mem, k), dim = -2)\n",
    "            v = torch.cat((v_xl_mem, v), dim = -2)\n",
    "\n",
    "        # calculate local attention\n",
    "\n",
    "        scale = self.scale.exp()\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k) * scale\n",
    "        i, j = sim.shape[-2:]\n",
    "\n",
    "        if exists(rel_pos_bias):\n",
    "            sim = rel_pos_bias[..., -i:, -j:] + sim\n",
    "\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n",
    "        sim = sim.masked_fill(causal_mask, mask_value)\n",
    "\n",
    "        # calculate knn attention over memory, if index is passed in\n",
    "\n",
    "        mem_kv, mem_mask = knn_memory.search(q, self.num_retrieved_memories)\n",
    "        mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "\n",
    "        sim_mem = einsum('b h i d, b h i j d -> b h i j', q, mem_k) * scale\n",
    "        sim_mem = sim_mem.masked_fill(~mem_mask, mask_value)\n",
    "\n",
    "        # calculate new XL memories, as well as memories to be discarded\n",
    "\n",
    "        new_kv_memories = torch.stack((k, v), dim = -2).detach()\n",
    "\n",
    "        if self.xl_max_memories > 0:\n",
    "            new_kv_memories_discarded, new_xl_kv_memories = new_kv_memories[:, :-self.xl_max_memories], new_kv_memories[:, -self.xl_max_memories:]\n",
    "        else:\n",
    "            new_kv_memories_discarded, new_xl_kv_memories = new_kv_memories, None\n",
    "\n",
    "        # add memories to be discarded into KNN memory\n",
    "\n",
    "        if add_knn_memory and new_kv_memories_discarded.numel() > 0:\n",
    "            knn_memory.add(new_kv_memories_discarded)\n",
    "\n",
    "        # attention (combining local and distant)\n",
    "\n",
    "        sim = torch.cat((sim_mem, sim), dim = -1)\n",
    "        attn = stable_softmax(sim)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        local_attn, mem_attn = attn[..., self.num_retrieved_memories:], attn[..., :self.num_retrieved_memories]\n",
    "        local_out = einsum('b h i j, b j d -> b h i d', local_attn, v)\n",
    "        mem_out = einsum('b h i j, b h i j d -> b h i d', mem_attn, mem_v)\n",
    "\n",
    "        out = local_out + mem_out\n",
    "\n",
    "        # combine heads and project out\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        # use flamingo styled gating of output, so that memorizing transformers can be gated into an existing LLM\n",
    "        # preparation to add this to block-recurrent-transformer-pytorch, for the pinnacle of long context attention network\n",
    "\n",
    "        if exists(self.output_gate):\n",
    "            out = out * self.output_gate.tanh()\n",
    "\n",
    "        return out, new_xl_kv_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPT(nn.Module):\n",
    "    def __init__(self, *, dim, patch_size, channels = 3):\n",
    "        super().__init__()\n",
    "        patch_dim = patch_size * patch_size * 5 * channels\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "        \n",
    "        # self.to_patch_embedding = nn.Sequential(\n",
    "        #     Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "        #     nn.LayerNorm(patch_dim),\n",
    "        #     nn.Linear(patch_dim, dim),\n",
    "        #     nn.LayerNorm(dim),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
    "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
    "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
    "        return self.to_patch_tokens(x_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemorizingSPTViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "\n",
    "        dim,\n",
    "        depth,\n",
    "\n",
    "        channels = 3,\n",
    "        \n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        knn_attn_heads = None,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.,\n",
    "        memorizing_layers = None,\n",
    "        max_knn_memories = 250000,\n",
    "        num_retrieved_memories = 32,\n",
    "        clear_memories_on_sos_token_id = None,\n",
    "        clear_memories_on_eos_token_id = None,\n",
    "        knn_memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY,\n",
    "        shift_knn_memories_down = 0.,\n",
    "        xl_max_memories = 0,\n",
    "        xl_memory_layers = None,\n",
    "        shift_xl_memories_down = 0.,\n",
    "        knn_memory_multiprocessing = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n",
    "        \n",
    "        # self.to_patch_embedding = nn.Sequential(\n",
    "        #     Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "        #     nn.LayerNorm(patch_dim),\n",
    "        #     nn.Linear(patch_dim, dim),\n",
    "        #     nn.LayerNorm(dim),\n",
    "        # )\n",
    "\n",
    "        block_wrapper = partial(PreNormResidual, dim)\n",
    "        valid_layers = set(range(1, depth + 1))\n",
    "\n",
    "        memorizing_layers = default(memorizing_layers, (depth // 2,)) # default KNN attention layer to midpoint of transformer\n",
    "        memorizing_layers = cast_tuple(memorizing_layers)\n",
    "        memorizing_layers = tuple(filter(lambda i: i in valid_layers, memorizing_layers))\n",
    "\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        knn_attn_heads = default(knn_attn_heads, heads)\n",
    "\n",
    "        # xl memory hyperparameter\n",
    "\n",
    "        if xl_max_memories > 0:\n",
    "            xl_memory_layers = default(xl_memory_layers, tuple(range(1, depth + 1)))\n",
    "            xl_memory_layers = unique(xl_memory_layers)\n",
    "            self.xl_memory_layers = tuple(filter(lambda i: i in valid_layers, xl_memory_layers))            \n",
    "            self.num_xl_memory_layers = len(self.xl_memory_layers)\n",
    "        else:\n",
    "            self.xl_memory_layers = tuple()\n",
    "            self.num_xl_memory_layers = 0\n",
    "\n",
    "        # knn memory hyperparameters\n",
    "\n",
    "        self.max_knn_memories = max_knn_memories\n",
    "        self.knn_memories_directory = knn_memories_directory\n",
    "        self.memorizing_layers = unique(memorizing_layers)\n",
    "        self.num_memory_layers = len(memorizing_layers)\n",
    "\n",
    "        self.clear_memories_on_sos_token_id = clear_memories_on_sos_token_id\n",
    "        self.clear_memories_on_eos_token_id = clear_memories_on_eos_token_id\n",
    "\n",
    "        # relative positional bias\n",
    "\n",
    "        self.rel_pos_bias = T5RelativePositionBias(scale = dim_head ** 0.5, heads = heads)\n",
    "        self.knn_rel_pos_bias = T5RelativePositionBias(scale = dim_head ** 0.5, heads = heads)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for idx in range(depth):\n",
    "            layer_num = idx + 1\n",
    "\n",
    "            use_xl_memories = layer_num in self.xl_memory_layers\n",
    "            use_knn_attention = layer_num in memorizing_layers\n",
    "            xl_max_memories_layer = 0 if not use_xl_memories else xl_max_memories\n",
    "\n",
    "            if use_knn_attention:\n",
    "                attn = KNNAttention(dim = dim, dim_head = dim_head, heads = knn_attn_heads, dropout = attn_dropout, num_retrieved_memories = num_retrieved_memories, xl_max_memories = xl_max_memories_layer)\n",
    "            else:\n",
    "                attn = Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, xl_max_memories = xl_max_memories_layer)\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                block_wrapper(attn),\n",
    "                block_wrapper(FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "        # memory layer shifting\n",
    "        # from a little known paper https://arxiv.org/abs/2012.15688\n",
    "\n",
    "        self.shift_knn_memories_down = shift_knn_memories_down\n",
    "        self.shift_xl_memories_down = shift_xl_memories_down\n",
    "\n",
    "        # to logits\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "        # knn memories init\n",
    "\n",
    "        self.knn_mem_kwargs = dict(\n",
    "            dim = self.dim_head,\n",
    "            max_memories = self.max_knn_memories,\n",
    "            multiprocessing = knn_memory_multiprocessing\n",
    "        )\n",
    "\n",
    "    def create_knn_memories(\n",
    "        self,\n",
    "        *,\n",
    "        batch_size\n",
    "    ):\n",
    "        return KNNMemoryList.create_memories(\n",
    "            batch_size = batch_size,\n",
    "            num_memory_layers = self.num_memory_layers,\n",
    "            memories_directory = self.knn_memories_directory,\n",
    "        )(**self.knn_mem_kwargs)\n",
    "\n",
    "    @contextmanager\n",
    "    def knn_memories_context(\n",
    "        self,\n",
    "        **kwargs\n",
    "    ):\n",
    "        knn_dir = Path(self.knn_memories_directory)\n",
    "        knn_dir.mkdir(exist_ok = True, parents = True)\n",
    "        lock = FileLock(str(knn_dir / 'mutex'))\n",
    "\n",
    "        with lock:\n",
    "            knn_memories = self.create_knn_memories(**kwargs)\n",
    "            yield knn_memories\n",
    "            knn_memories.cleanup()\n",
    "\n",
    "    def clear_memory(self, x, token_id, knn_memories):\n",
    "        \"\"\" clears the KNN memories based on if the batch row contains the specified token id \"\"\"\n",
    "        \"\"\" for auto-clearing KNN memories based on start and end of strings \"\"\"\n",
    "\n",
    "        clear_memory = (x == token_id).any(dim = -1)\n",
    "        batch_indices, _ = clear_memory.nonzero(as_tuple = True)\n",
    "        batch_indices_to_clear = batch_indices.tolist()\n",
    "\n",
    "        if len(batch_indices_to_clear) == 0:\n",
    "            return\n",
    "\n",
    "        knn_memories.clear_memory(batch_indices_to_clear)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        knn_memories,\n",
    "        xl_memories    = None,\n",
    "        add_knn_memory = True\n",
    "    ):\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        \n",
    "        *_, h, w, dtype = *x.shape, x.dtype\n",
    "\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "        \n",
    "        batch_size, seq_len, *_, device = *x.shape, x.device\n",
    "\n",
    "        # validate KNN memories to have enough indices for batch size\n",
    "\n",
    "        assert all([memory.num_indices == batch_size for memory in knn_memories]), f'you passed in an input with batch size {batch_size} but your memories were not instantiated with that number of KNN indices'\n",
    "\n",
    "        # if KNN memories are passed in, and researcher wants memories auto-cleared on <sos> token detection\n",
    "        # do the appropriate logic\n",
    "\n",
    "        if exists(self.clear_memories_on_sos_token_id):\n",
    "            self.clear_memory(x, self.clear_memories_on_sos_token_id, knn_memories)\n",
    "\n",
    "        # handle XL memories\n",
    "\n",
    "        xl_memories = default(xl_memories, (None,) * self.num_xl_memory_layers)\n",
    "        assert len(xl_memories) == self.num_xl_memory_layers\n",
    "        has_xl_memories = len(xl_memories) > 0\n",
    "\n",
    "        # shifting memories a number of layers down, little known technique shown to enhance memories from Ernie-Doc paper\n",
    "\n",
    "        if len(knn_memories) > 0 and self.shift_knn_memories_down > 0:\n",
    "            knn_memories = [*knn_memories[self.shift_knn_memories_down:], *knn_memories[:self.shift_knn_memories_down]]\n",
    "\n",
    "        if len(xl_memories) > 0 and self.shift_xl_memories_down > 0:\n",
    "            xl_memories = [*xl_memories[self.shift_xl_memories_down:], *xl_memories[:self.shift_xl_memories_down]]\n",
    "\n",
    "        # iterate through the memories in order of the ascending layers that contain KNNAttention\n",
    "\n",
    "        xl_memories_iter = iter(xl_memories)\n",
    "        knn_memories_iter = iter(knn_memories)\n",
    "\n",
    "        # positional bias\n",
    "\n",
    "        max_context_len = max([seq_len, *map(lambda t: (t.shape[-3] if exists(t) else 0) + seq_len, xl_memories)])\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias(seq_len, max_context_len, device = device)\n",
    "        knn_rel_pos_bias = self.knn_rel_pos_bias(seq_len, max_context_len, device = device)\n",
    "\n",
    "        # keep track of new xl memories\n",
    "\n",
    "        new_xl_memories = [] if has_xl_memories else None\n",
    "\n",
    "        # go through all layers\n",
    "\n",
    "        for ind, (attn, ff) in enumerate(self.layers):\n",
    "            layer_num = ind + 1\n",
    "\n",
    "            is_memorizing_layer = layer_num in self.memorizing_layers\n",
    "            is_xl_memory_layer = layer_num in self.xl_memory_layers\n",
    "\n",
    "            attn_kwargs = dict(rel_pos_bias = rel_pos_bias if not is_memorizing_layer else knn_rel_pos_bias)\n",
    "\n",
    "            if is_memorizing_layer:\n",
    "                attn_kwargs = {**attn_kwargs, 'knn_memory': next(knn_memories_iter), 'add_knn_memory': add_knn_memory}\n",
    "\n",
    "            if is_xl_memory_layer:\n",
    "                attn_kwargs = {**attn_kwargs, 'xl_memory': next(xl_memories_iter)}\n",
    "\n",
    "            # attention\n",
    "\n",
    "            x, xl_mem = attn(x, **attn_kwargs)\n",
    "\n",
    "            # add new XL memories if needed\n",
    "\n",
    "            if exists(xl_mem):\n",
    "                new_xl_memories.append(xl_mem)\n",
    "\n",
    "            # feedforward\n",
    "\n",
    "            x = ff(x)\n",
    "\n",
    "        x = x.mean(dim = 1)\n",
    "        #x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        #x = x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        logits = self.to_logits(x)\n",
    "\n",
    "        # auto-clear KNN memories on end of token\n",
    "\n",
    "        if exists(self.clear_memories_on_eos_token_id):\n",
    "            self.clear_memory(x, self.clear_memories_on_eos_token_id, knn_memories)\n",
    "\n",
    "        if exists(new_xl_memories):\n",
    "            return logits, new_xl_memories\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "# import glob\n",
    "# from itertools import chain\n",
    "# import os\n",
    "# import random\n",
    "# import zipfile\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "sizes      = 224\n",
    "epochs     = 5\n",
    "lr         = 3e-5\n",
    "gamma      = 0.9\n",
    "seed       = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile('train.zip') as train_zip:\n",
    "#    train_zip.extractall('data')\n",
    "    \n",
    "#with zipfile.ZipFile('test.zip') as test_zip:\n",
    "#    test_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob.glob(os.path.join(train_dir,'*.jpg'))\n",
    "test_list = glob.glob(os.path.join(test_dir, '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Data: {len(train_list)}\")\n",
    "print(f\"Test Data: {len(test_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [path.split('/')[-1].split('.')[0] for path in train_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(1, len(train_list), size=9)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    img = Image.open(train_list[idx])\n",
    "    ax.set_title(labels[idx])\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, valid_list = train_test_split(train_list, \n",
    "                                          test_size=0.2,\n",
    "                                          stratify=labels,\n",
    "                                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Data: {len(train_list)}\")\n",
    "print(f\"Validation Data: {len(valid_list)}\")\n",
    "print(f\"Test Data: {len(test_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(sizes),\n",
    "        transforms.GaussianBlur(kernel_size=(11,11), sigma=(0.05, 2.)),\n",
    "        transforms.RandomAffine(0, shear=10,scale=(0.8, 1.2)),\n",
    "        transforms.RandomPhotometricDistort(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomErasing(),\n",
    "        transforms.ToImageTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),\n",
    "            (0.229, 0.224, 0.225)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(sizes),\n",
    "        transforms.CenterCrop(sizes),\n",
    "        transforms.ToImageTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),\n",
    "            (0.229, 0.224, 0.225)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(sizes),\n",
    "        transforms.CenterCrop(sizes),\n",
    "        transforms.ToImageTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        label = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        label = 1 if label == \"dog\" else 0\n",
    "\n",
    "        return img_transformed, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CatsDogsDataset(train_list, transform=train_transforms)\n",
    "valid_data = CatsDogsDataset(valid_list, transform=test_transforms)\n",
    "test_data = CatsDogsDataset(test_list, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True )\n",
    "test_loader  = DataLoader(dataset = test_data,  batch_size=batch_size, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(train_loader))\n",
    "print(len(valid_data), len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loader,\n",
    "    metric,\n",
    "    epochs,\n",
    "    phase_train = 'train',\n",
    "    max_grad_clip_norm = 0.5\n",
    "):\n",
    "    for epoch in range(int(len(metric)/2), int(len(metric)/2)+epochs):\n",
    "        for phase in loader:\n",
    "\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "\n",
    "            if phase == phase_train:\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            for data, label in tqdm(loader[phase]):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == phase_train):\n",
    "                    with model.knn_memories_context(batch_size = data.size()[0]) as knn_memories:\n",
    "                        output = model(\n",
    "                            data,\n",
    "                            knn_memories = knn_memories\n",
    "                        )\n",
    "                        loss = criterion(output, label)\n",
    "                        if phase == phase_train:\n",
    "                            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_clip_norm)\n",
    "                            loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_clip_norm)\n",
    "                            optimizer.step()\n",
    "\n",
    "                        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "\n",
    "                        epoch_accuracy += acc / len(loader[phase])\n",
    "                        epoch_loss += loss / len(loader[phase])\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} Phase: {phase} - loss: {epoch_loss:.4f} - acc: {epoch_accuracy:.4f}\"\n",
    "            )\n",
    "            metric.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"phase\": phase,\n",
    "                \"loss\": epoch_loss.item(),\n",
    "                \"accuracy\": epoch_accuracy.item()\n",
    "            })\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test SPTViT without Memorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MemorizingSPTViT(\n",
    "#     image_size = sizes,                 # Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "#     patch_size = 32,                    # Number of patches. image_size must be divisible by patch_size. The number of patches is:  n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "#     num_classes = 2,                    # Number of classes to classify.\n",
    "#     heads = 16,\n",
    "#     dim = 1024,                         # Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "#     dim_head = 32,                      # dimension per attention head\n",
    "#     depth = 8,                          # number of layers\n",
    "    \n",
    "#     #pool = 'cls',\n",
    "    \n",
    "#     memorizing_layers = (),             # which layers to have ANN memories\n",
    "#     max_knn_memories = 512*64,          # maximum ANN memories to keep (once it hits this capacity, it will be reset for now, due to limitations in faiss' ability to remove entries)\n",
    "#     num_retrieved_memories = 64,        # number of ANN memories to retrieve\n",
    "#     clear_memories_on_sos_token_id = None, # clear passed in ANN memories automatically for batch indices which contain this specified SOS token id - otherwise, you can also manually iterate through the ANN memories and clear the indices before the next iteration\n",
    " \n",
    "#     xl_memory_layers = None,\n",
    "#     xl_max_memories = 0,\n",
    "\n",
    "#     shift_knn_memories_down = 0,\n",
    "#     shift_xl_memories_down = 0,\n",
    "\n",
    "#     knn_attn_heads = None,\n",
    "#     attn_dropout = 0.1,\n",
    "#     ff_mult = 4,\n",
    "#     ff_dropout = 0.1,\n",
    "#     clear_memories_on_eos_token_id = None,\n",
    "#     knn_memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY,\n",
    "    \n",
    "#     knn_memory_multiprocessing = True\n",
    "\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"pretrained-net-without-mem-240.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "# # scheduler\n",
    "# scheduler = StepLR(optimizer, step_size=3, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader={\n",
    "#     'train':train_loader,\n",
    "#     'val':valid_loader\n",
    "# }\n",
    "\n",
    "# net_without_mem=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('net-without-mem-100.pickle', 'rb') as handle:\n",
    "#     net_without_mem = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(40):\n",
    "#     epoch=train(\n",
    "#         model=model,\n",
    "#         criterion=criterion,\n",
    "#         optimizer=optimizer,\n",
    "#         scheduler=scheduler,\n",
    "#         loader=loader,\n",
    "#         metric=net_without_mem,\n",
    "#         epochs=epochs,\n",
    "#     )\n",
    "#     torch.save(model.state_dict(), f'pretrained-net-without-mem-{epoch+1}.pt')\n",
    "#     with open(f'net-without-mem-{epoch+1}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(net_without_mem, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('net-without-mem-125.pickle', 'rb') as handle:\n",
    "    net_without_mem = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_net_without_mem_val, acc_net_without_mem_val = zip(*[(k[\"loss\"], k[\"accuracy\"]) for k in net_without_mem if k[\"phase\"]=='val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_net_without_mem_train, acc_net_without_mem_train = zip(*[(k[\"loss\"], k[\"accuracy\"]) for k in net_without_mem if k[\"phase\"]=='train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.title(\"Net without memory\")\n",
    "\n",
    "plt.plot(loss_net_without_mem_val, label = \"Loss Val\")\n",
    "\n",
    "plt.plot(acc_net_without_mem_val, label = \"Accuracy Val\")\n",
    "\n",
    "plt.plot(loss_net_without_mem_train, label = \"Loss Train\")\n",
    "\n",
    "plt.plot(acc_net_without_mem_train, label = \"Accuracy Train\")\n",
    "\n",
    "plt.axvline(x = 100, color = 'lightgray', linestyle=\"dashed\", label = 'Start mem train')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SPTViT Finetuning with Memorizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemorizingSPTViT(\n",
    "    image_size = sizes,                 # Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "    patch_size = 32,                    # Number of patches. image_size must be divisible by patch_size. The number of patches is:  n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "    num_classes = 2,                    # Number of classes to classify.\n",
    "    heads = 16,\n",
    "    dim = 1024,                         # Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "    dim_head = 32,                      # dimension per attention head\n",
    "    depth = 8,                          # number of layers\n",
    "    \n",
    "    #pool = 'cls',\n",
    "    \n",
    "    memorizing_layers = (4, 5),         # which layers to have ANN memories\n",
    "    max_knn_memories = 512*64,          # maximum ANN memories to keep (once it hits this capacity, it will be reset for now, due to limitations in faiss' ability to remove entries)\n",
    "    num_retrieved_memories = 64,        # number of ANN memories to retrieve\n",
    "    clear_memories_on_sos_token_id = None, # clear passed in ANN memories automatically for batch indices which contain this specified SOS token id - otherwise, you can also manually iterate through the ANN memories and clear the indices before the next iteration\n",
    "    \n",
    "    xl_memory_layers = None,\n",
    "    xl_max_memories = 0,\n",
    "\n",
    "    shift_knn_memories_down = 0,\n",
    "    shift_xl_memories_down = 0,\n",
    "\n",
    "    knn_attn_heads = None,\n",
    "    attn_dropout = 0.1,\n",
    "    ff_mult = 4,\n",
    "    ff_dropout = 0.1,\n",
    "    clear_memories_on_eos_token_id = None,\n",
    "    knn_memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY,\n",
    "    \n",
    "    knn_memory_multiprocessing = True\n",
    "\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"pretrained-net-without-mem-100.pt\"), strict=False)\n",
    "model.load_state_dict(torch.load(\"pretrained-net-with-mem-110.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader={\n",
    "    'train':train_loader,\n",
    "    'val':valid_loader\n",
    "}\n",
    "\n",
    "# net_with_mem=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_with_mem = copy.deepcopy(net_without_mem[:200])\n",
    "with open('net-with-mem-110.pickle', 'rb') as handle:\n",
    "    net_with_mem = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    epoch=train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loader=loader,\n",
    "        metric=net_with_mem,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    torch.save(model.state_dict(), f'pretrained-net-with-mem-{epoch+1}.pt')\n",
    "    with open(f'net-with-mem-{epoch+1}.pickle', 'wb') as handle:\n",
    "        pickle.dump(net_with_mem, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('net-with-mem-110.pickle', 'rb') as handle:\n",
    "    net_with_mem = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_net_with_mem_val, acc_net_with_mem_val = zip(*[(k[\"loss\"], k[\"accuracy\"]) for k in net_with_mem if k[\"phase\"]=='val'])\n",
    "loss_net_with_mem_train, acc_net_with_mem_train = zip(*[(k[\"loss\"], k[\"accuracy\"]) for k in net_with_mem if k[\"phase\"]=='train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Net with memory Train\")\n",
    "\n",
    "# plt.plot(loss_net_with_mem_val, label = \"LossVal+mem\")\n",
    "\n",
    "# plt.plot(acc_net_with_mem_val, label = \"AccuracyVal+mem\")\n",
    "\n",
    "plt.plot(loss_net_with_mem_train, label = \"LossTrain+mem\")\n",
    "\n",
    "plt.plot(acc_net_with_mem_train, label = \"AccuracyTrain+mem\")\n",
    "\n",
    "# loss_net_without_mem_val\n",
    "\n",
    "# plt.plot(loss_net_without_mem_val, label = \"Loss Val\", alpha=0.2)\n",
    "\n",
    "# plt.plot(acc_net_without_mem_val, label = \"Accuracy Val\", alpha=0.2)\n",
    "\n",
    "plt.plot(loss_net_without_mem_train, label = \"Loss Train\", alpha=0.2)\n",
    "\n",
    "plt.plot(acc_net_without_mem_train, label = \"Accuracy Train\", alpha=0.2)\n",
    "\n",
    "#plt.axvline(x = 100, color = 'lightgray', linestyle=\"dashed\", label = 'Start mem train')\n",
    "\n",
    "plt.legend(loc=\"lower left\", ncol=1)\n",
    "\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(0, 120)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Net with memory Val\")\n",
    "\n",
    "plt.plot(loss_net_with_mem_val, label = \"LossVal+mem\")\n",
    "\n",
    "plt.plot(acc_net_with_mem_val, label = \"AccuracyVal+mem\")\n",
    "\n",
    "# plt.plot(loss_net_with_mem_train, label = \"LossTrain+mem\")\n",
    "\n",
    "# plt.plot(acc_net_with_mem_train, label = \"AccuracyTrain+mem\")\n",
    "\n",
    "# loss_net_without_mem_val\n",
    "\n",
    "plt.plot(loss_net_without_mem_val, label = \"Loss Val\", alpha=0.2)\n",
    "\n",
    "plt.plot(acc_net_without_mem_val, label = \"Accuracy Val\", alpha=0.2)\n",
    "\n",
    "# plt.plot(loss_net_without_mem_train, label = \"Loss Train\", alpha=0.2)\n",
    "\n",
    "# plt.plot(acc_net_without_mem_train, label = \"Accuracy Train\", alpha=0.2)\n",
    "\n",
    "#plt.axvline(x = 100, color = 'lightgray', linestyle=\"dashed\", label = 'Start mem train')\n",
    "\n",
    "plt.legend(loc=\"lower left\", ncol=1)\n",
    "\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(0, 120)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SPTViT with Memorizing and XL memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MemorizingSPTViT(\n",
    "#     image_size = sizes,                 # Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "#     patch_size = 32,                    # Number of patches. image_size must be divisible by patch_size. The number of patches is:  n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "#     num_classes = 2,                    # Number of classes to classify.\n",
    "#     heads = 16,\n",
    "#     dim = 1024,                         # Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "#     dim_head = 32,                      # dimension per attention head\n",
    "#     depth = 8,                          # number of layers\n",
    "    \n",
    "#     #pool = 'cls',\n",
    "    \n",
    "#     memorizing_layers = (4, 5),      # which layers to have ANN memories\n",
    "#     max_knn_memories = 512*64,          # maximum ANN memories to keep (once it hits this capacity, it will be reset for now, due to limitations in faiss' ability to remove entries)\n",
    "#     num_retrieved_memories = 64,        # number of ANN memories to retrieve\n",
    "#     clear_memories_on_sos_token_id = None, # clear passed in ANN memories automatically for batch indices which contain this specified SOS token id - otherwise, you can also manually iterate through the ANN memories and clear the indices before the next iteration\n",
    "    \n",
    "#     xl_memory_layers = (7, 8),\n",
    "#     xl_max_memories = 1024,\n",
    "\n",
    "#     shift_knn_memories_down = 0,\n",
    "#     shift_xl_memories_down = 0,\n",
    "\n",
    "#     knn_attn_heads = None,\n",
    "#     attn_dropout = 0.1,\n",
    "#     ff_mult = 4,\n",
    "#     ff_dropout = 0.1,\n",
    "#     clear_memories_on_eos_token_id = None,\n",
    "#     knn_memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY,\n",
    "    \n",
    "#     knn_memory_multiprocessing = True\n",
    "\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# # scheduler\n",
    "# scheduler = StepLR(optimizer, step_size=3, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader={\n",
    "#     'train':train_loader,\n",
    "#     'val':valid_loader\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_with_mem_xl=[]\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     xl_memories = None\n",
    "#     for phase in loader:\n",
    "        \n",
    "#         epoch_loss = 0\n",
    "#         epoch_accuracy = 0\n",
    "        \n",
    "#         if phase == 'train':\n",
    "#             model.train()  # Set model to training mode\n",
    "#         else:\n",
    "#             model.eval()   # Set model to evaluate mode\n",
    "        \n",
    "#         for data, label in tqdm(loader[phase]):\n",
    "#             data = data.to(device)\n",
    "#             label = label.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             if phase != 'train':\n",
    "#                 xl_memories = None\n",
    "            \n",
    "#             with torch.set_grad_enabled(phase == 'train'):\n",
    "#                 with model.knn_memories_context(batch_size = data.size()[0]) as knn_memories:\n",
    "#                     output, xl_memories = model(\n",
    "#                         data,\n",
    "#                         knn_memories = knn_memories,\n",
    "#                         xl_memories = xl_memories\n",
    "#                     )\n",
    "#                     loss = criterion(output, label)\n",
    "#                     loss = criterion(output, label)\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                     acc = (output.argmax(dim=1) == label).float().mean()\n",
    "\n",
    "#                     epoch_accuracy += acc / len(loader[phase])\n",
    "#                     epoch_loss += loss / len(loader[phase])\n",
    "#         print(\n",
    "#             f\"Epoch: {epoch+1} Phase: {phase} - loss: {epoch_loss:.4f} - acc: {epoch_accuracy:.4f}\"\n",
    "#         )\n",
    "#         net_with_mem_xl.append({\n",
    "#             \"epoch\": epoch,\n",
    "#             \"phase\": phase,\n",
    "#             \"loss\": epoch_loss,\n",
    "#             \"accuracy\": epoch_accuracy\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'pretrained-net-with-mem-xl.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
